{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9427d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CodSoft Internship Programme for Data Science (1 October 2023 to 31 October 2023) #####\n",
    "##### Name:- Deepak Gupta #####\n",
    "##### Task-5- Credit Card Faurad Detection ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df73e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Importing modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff08b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### read the dataset\n",
    "dataset = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "##### read the first 5 and last 5 rows of the data\n",
    "dataset.head().append(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### check for relative proportion \n",
    "print(\"Fraudulent Cases: \" + str(len(dataset[dataset[\"Class\"] == 1])))\n",
    "print(\"Valid Transactions: \" + str(len(dataset[dataset[\"Class\"] == 0])))\n",
    "print(\"Proportion of Fraudulent Cases: \" + str(len(dataset[dataset[\"Class\"] == 1])/ dataset.shape[0]))\n",
    "\n",
    "##### To see how small are the number of Fraud transactions\n",
    "data_p = dataset.copy()\n",
    "data_p[\" \"] = np.where(data_p[\"Class\"] == 1 ,  \"Fraud\", \"Genuine\")\n",
    "\n",
    "##### plot a pie chart\n",
    "data_p[\" \"].value_counts().plot(kind=\"pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot the named features \n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(18,4), sharex = True)\n",
    "\n",
    "amount_value = dataset['Amount'].values # values\n",
    "time_value = dataset['Time'].values # values\n",
    "\n",
    "sns.distplot(amount_value, hist=False, color=\"m\", kde_kws={\"shade\": True}, ax=axes[0]).set_title('Distribution of Amount')\n",
    "sns.distplot(time_value, hist=False, color=\"m\", kde_kws={\"shade\": True}, ax=axes[1]).set_title('Distribution of Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Amount in a Fraudulent Transaction: \" + str(dataset[dataset[\"Class\"] == 1][\"Amount\"].mean()))\n",
    "print(\"Average Amount in a Valid Transaction: \" + str(dataset[dataset[\"Class\"] == 0][\"Amount\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary of the feature - Amount\" + \"\\n-------------------------------\")\n",
    "print(dataset[\"Amount\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Reorder the columns Amount, Time then the rest\n",
    "data_plot = dataset.copy()\n",
    "amount = data_plot['Amount']\n",
    "data_plot.drop(labels=['Amount'], axis=1, inplace = True)\n",
    "data_plot.insert(0, 'Amount', amount)\n",
    "\n",
    "##### Plot the distributions of the features\n",
    "columns = data_plot.iloc[:,0:30].columns\n",
    "plt.figure(figsize=(12,30*4))\n",
    "grids = gridspec.GridSpec(30, 1)\n",
    "for grid, index in enumerate(data_plot[columns]):\n",
    " ax = plt.subplot(grids[grid])\n",
    " sns.distplot(data_plot[index][data_plot.Class == 1], hist=False, kde_kws={\"shade\": True}, bins=50)\n",
    " sns.distplot(data_plot[index][data_plot.Class == 0], hist=False, kde_kws={\"shade\": True}, bins=50)\n",
    " ax.set_xlabel(\"\")\n",
    " ax.set_title(\"Distribution of Column: \"  + str(index))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedce25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### check for null values\n",
    "\n",
    "dataset.isnull().shape[0]\n",
    "print(\"Non-missing values: \" + str(dataset.isnull().shape[0]))\n",
    "print(\"Missing values: \" + str(dataset.shape[0] - dataset.isnull().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler().fit(dataset[[\"Time\", \"Amount\"]])\n",
    "dataset[[\"Time\", \"Amount\"]] = scaler.transform(dataset[[\"Time\", \"Amount\"]])\n",
    "\n",
    "dataset.head().append(dataset.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe07f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Separate response and features  Undersampling before cross validation will lead to overfiting\n",
    "\n",
    "y = dataset[\"Class\"] # target \n",
    "X = dataset.iloc[:,0:30]\n",
    "\n",
    "##### Use SKLEARN for the split\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "        X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create the cross validation framework \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, random_state = None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Import the imbalance Learn module\n",
    "from imblearn.pipeline import make_pipeline ## Create a Pipeline using the provided estimators .\n",
    "from imblearn.under_sampling import NearMiss  ## perform Under-sampling  based on NearMiss methods. \n",
    "from imblearn.over_sampling import SMOTE  ## PerformOver-sampling class that uses SMOTE. \n",
    "\n",
    "##### import the metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "##### Import the classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49a3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Fit and predict\n",
    "rfc = RandomForestClassifier() \n",
    "rfc.fit(X_train, y_train) \n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "##### For the performance let's use some metrics from SKLEARN module\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "  \n",
    "print(\"The accuracy is\", accuracy_score(y_test, y_pred)) \n",
    "print(\"The precision is\", precision_score(y_test, y_pred))\n",
    "print(\"The recall is\", recall_score(y_test, y_pred))\n",
    "print(\"The F1 score is\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e57094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_best_estimator_and_metrics(estimator, params, kf=kf, X_train=X_train, \n",
    "                                         y_train=y_train, X_test=X_test, \n",
    "                                         y_test=y_test, is_grid_search=True, \n",
    "                                         sampling=NearMiss(), scoring=\"f1\", \n",
    "                                         n_jobs=2):\n",
    "    if sampling is None:\n",
    "        # make the pipeline of only the estimator, just so the remaining code will work fine\n",
    "        pipeline = make_pipeline(estimator)\n",
    "    else:\n",
    "        # make the pipeline of over/undersampling and estimator\n",
    "        pipeline = make_pipeline(sampling, estimator)\n",
    "    # get the estimator name\n",
    "    estimator_name = estimator.__class__.__name__.lower()\n",
    "    # construct the parameters for grid/random search cv\n",
    "    new_params = {f'{estimator_name}__{key}': params[key] for key in params}\n",
    "    if is_grid_search:\n",
    "        # grid search instead of randomized search\n",
    "        search = GridSearchCV(pipeline, param_grid=new_params, cv=kf, return_train_score=True, n_jobs=n_jobs, verbose=2)\n",
    "    else:\n",
    "        # randomized search\n",
    "        search = RandomizedSearchCV(pipeline, param_distributions=new_params, \n",
    "                                    cv=kf, scoring=scoring, return_train_score=True,\n",
    "                                    n_jobs=n_jobs, verbose=1)\n",
    "    # fit the model\n",
    "    search.fit(X_train, y_train)\n",
    "    cv_score = cross_val_score(search, X_train, y_train, scoring=scoring, cv=kf)\n",
    "    # make predictions on the test data\n",
    "    y_pred = search.best_estimator_.named_steps[estimator_name].predict(X_test)\n",
    "    # calculate the metrics: recall, accuracy, F1 score, etc.\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    y_proba = search.best_estimator_.named_steps[estimator_name].predict_proba(X_test)[::, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    # return the best estimator along with the metrics\n",
    "    return {\n",
    "        \"best_estimator\": search.best_estimator_,\n",
    "        \"estimator_name\": estimator_name,\n",
    "        \"cv_score\": cv_score,\n",
    "        \"recall\": recall,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"fpr\": fpr,\n",
    "        \"tpr\": tpr,\n",
    "        \"auc\": auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82967f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cumulatively create a table for the ROC curve\n",
    "###### Create the dataframe\n",
    "res_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "\n",
    "logreg_us_results = get_model_best_estimator_and_metrics(\n",
    "    estimator=LogisticRegression(),\n",
    "    params={\"penalty\": ['l1', 'l2'], \n",
    "                  'C': [ 0.01, 0.1, 1, 100], \n",
    "                  'solver' : ['liblinear']},\n",
    "    sampling=NearMiss(),\n",
    ")\n",
    "print(f\"==={logreg_us_results['estimator_name']}===\")\n",
    "print(\"Model:\", logreg_us_results['best_estimator'])\n",
    "print(\"Accuracy:\", logreg_us_results['accuracy'])\n",
    "print(\"Recall:\", logreg_us_results['recall'])\n",
    "print(\"F1 Score:\", logreg_us_results['f1_score'])\n",
    "res_table = res_table.append({'classifiers': logreg_us_results[\"estimator_name\"],\n",
    "                                        'fpr': logreg_us_results[\"fpr\"], \n",
    "                                        'tpr': logreg_us_results[\"tpr\"], \n",
    "                                        'auc': logreg_us_results[\"auc\"]\n",
    "                              }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d2307",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plot the ROC curve for undersampling\n",
    "\n",
    "res_table.set_index('classifiers', inplace=True)\n",
    "fig = plt.figure(figsize=(17,7))\n",
    "\n",
    "for j in res_table.index:\n",
    "    plt.plot(res_table.loc[j]['fpr'], \n",
    "             res_table.loc[j]['tpr'], \n",
    "             label=\"{}, AUC={:.3f}\".format(j, res_table.loc[j]['auc']))\n",
    "    \n",
    "plt.plot([0,1], [0,1], color='orange', linestyle='--')\n",
    "plt.xticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.xlabel(\"Positive Rate(False)\", fontsize=15)\n",
    "plt.yticks(np.arange(0.0, 1.1, step=0.1))\n",
    "plt.ylabel(\"Positive Rate(True)\", fontsize=15)\n",
    "plt.title('Analysis for Oversampling', fontweight='bold', fontsize=15)\n",
    "plt.legend(prop={'size':13}, loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Cumulatively create a table for the ROC curve\n",
    "\n",
    "res_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n",
    "\n",
    "lin_reg_os_results = get_model_best_estimator_and_metrics(\n",
    "    estimator=LogisticRegression(),\n",
    "    params={\"penalty\": ['l1', 'l2'], 'C': [ 0.01, 0.1, 1, 100, 100], \n",
    "            'solver' : ['liblinear']},\n",
    "    sampling=SMOTE(random_state=42),\n",
    "    scoring=\"f1\",\n",
    "    is_grid_search=False,\n",
    "    n_jobs=2,\n",
    ")\n",
    "print(f\"==={lin_reg_os_results['estimator_name']}===\")\n",
    "print(\"Model:\", lin_reg_os_results['best_estimator'])\n",
    "print(\"Accuracy:\", lin_reg_os_results['accuracy'])\n",
    "print(\"Recall:\", lin_reg_os_results['recall'])\n",
    "print(\"F1 Score:\", lin_reg_os_results['f1_score'])\n",
    "res_table = res_table.append({'classifiers': lin_reg_os_results[\"estimator_name\"],\n",
    "                                        'fpr': lin_reg_os_results[\"fpr\"], \n",
    "                                        'tpr': lin_reg_os_results[\"tpr\"], \n",
    "                                        'auc': lin_reg_os_results[\"auc\"]\n",
    "                              }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3924c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### boxplot for two example variables in the dataset\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(18,4), sharex = True)\n",
    "\n",
    "variable1 = dataset[\"V1\"]\n",
    "variable2 = dataset[\"V2\"]\n",
    "\n",
    "sns.boxplot(variable1, color=\"m\", ax=axes[0]).set_title('Boxplot for V1')\n",
    "sns.boxplot(variable2, color=\"m\", ax=axes[1]).set_title('Boxplot for V2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Find the IQR for all the feature variables\n",
    "##### Please note that we are keeping Class variable also in this evaluation, though we know using this method no observation\n",
    "##### be removed based on this variable.\n",
    "\n",
    "quartile1 = dataset.quantile(0.25)\n",
    "quartile3 = dataset.quantile(0.75)\n",
    "\n",
    "IQR = quartile3 - quartile1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1459c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Remove the outliers \n",
    "constant = 3\n",
    "datavalid = dataset[~((dataset < (quartile1 - constant * IQR)) |(dataset > (quartile3 + constant * IQR))).any(axis=1)]\n",
    "deletedrows = dataset.shape[0] - datavalid.shape[0]\n",
    "print(\"We have removed \" + str(deletedrows) + \" rows from the data as outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eedb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551dc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcc166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
